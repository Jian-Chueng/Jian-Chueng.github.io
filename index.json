[{"authors":["admin"],"categories":null,"content":"Jian Zhang is currently a Ph.D. Candidate with the School of Electrical Engineering and Telecommunications, University of New South Wales (UNSW), Australia. She received her B.Sc. (Magna Cum Laude) degree in Electrical Engineering in 2016 from Kennesaw State University, Georgia, USA. Her current research interests include machine learning and intelligent control, hybrid systems, navigation, and control of mobile robots.\n","date":1575158400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1575158400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://jianzhang6.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Jian Zhang is currently a Ph.D. Candidate with the School of Electrical Engineering and Telecommunications, University of New South Wales (UNSW), Australia. She received her B.Sc. (Magna Cum Laude) degree in Electrical Engineering in 2016 from Kennesaw State University, Georgia, USA. Her current research interests include machine learning and intelligent control, hybrid systems, navigation, and control of mobile robots.","tags":null,"title":"Jian Zhang","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic's Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://jianzhang6.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Jian Zhang"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"5f48c44b48f391da9cec869df8bee73b","permalink":"https://jianzhang6.github.io/publication/conference-paper/anzcc2019/1/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/conference-paper/anzcc2019/1/","section":"publication","summary":"This work presents a collision-free 3D path planning strategy for the non-holonomic mobile robot. The non-holonomic mobile robot travels through an unknown 3D environment with obstacles and reaches a given destination safely with no collisions. In addition, our approach enables to find the optimal (shortest) path to the target efficiently based on the avoiding plane selected.","tags":["Obstacle Avoidance","Reactive Navigation","Shortest Path Planning","3D Environments","Reinforcement Learning"],"title":"A Collision-Free 3D Path Planning Strategy for Mobile Robots","type":"publication"},{"authors":["Jian Zhang"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"46e930f7b15050bf0f81b0a3d40bf015","permalink":"https://jianzhang6.github.io/publication/conference-paper/anzcc2019/2/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/conference-paper/anzcc2019/2/","section":"publication","summary":"This study develops a new path planning method which utilizes integrated environment representation and reinforcement learning to control a mobile robot with non-holonomic constraints in unknown dynamic environments. With the control algorithm presented, no approximating the shapes of the obstacles or even any information about the obstacles' velocities is needed. Our novel approach enables to find the optimal path to the target efficiently and avoid collisions in a cluttered environment with steady and moving obstacles.","tags":["Obstacle Avoidance","Reactive Navigation","Integrated Environment Representation","Dynamic Environments","Reinforcement Learning"],"title":"Path Planning for a Mobile Robot in Unknown Dynamic Environments Using Integrated Environment Representation and Reinforcement Learning","type":"publication"},{"authors":["Jian Zhang"],"categories":null,"content":"I am truly humbled and honored to receive the ANZCC 2019 best paper award. This is clearly one of the most memorable days of my Ph.D. study!\nMy deepest thanks for ANZCC's encouragementÔºÅ\nüòâ\n","date":1574985600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574985600,"objectID":"e63aaeb581247131065a77d16bdfd0de","permalink":"https://jianzhang6.github.io/post/anzcc/","publishdate":"2019-11-29T00:00:00Z","relpermalink":"/post/anzcc/","section":"post","summary":"I am truly humbled and honored to receive the ANZCC 2019 best paper award.","tags":["Êó•Â∏∏"],"title":"ANZCC 2019 Best Paper ü•≥","type":"post"},{"authors":["Jian Zhang"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"0bd1928eaca27d58bf76b945a5a96cbc","permalink":"https://jianzhang6.github.io/publication/conference-paper/ccc2019/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/conference-paper/ccc2019/","section":"publication","summary":"The reactive collision-free navigation problems are challenging due to the limitation of the environment information. In this paper, we propose a novel hybrid reactive navigation strategy for non-holonomic mobile robots in cluttered environments. Our strategy combines both reactive navigation and Q-learning method.","tags":["Cluttered Environments","Obstacle Avoidance","Reactive Navigation","Shortest Path Planning","Q-Learning","Reinforcement Learning"],"title":"A Hybrid Reactive Navigation Strategy for a Non-holonomic Mobile Robot in Cluttered Environments","type":"publication"},{"authors":["Jian Zhang"],"categories":[],"content":"import numpy as np import matplotlib.pyplot as plt from sklearn import datasets import keras from keras.models import Sequential from keras.layers import Dense from keras.optimizers import Adam from keras.utils.np_utils import to_categorical  n_pts = 500 centers = [[-1, 1], [-1, -1], [1, -1], [1, 1], [0, 0]] X, y = datasets.make_blobs(n_samples=n_pts, random_state=123, centers=centers, cluster_std=0.4)  plt.scatter(X[y==0, 0],X[y==0, 1]) plt.scatter(X[y==1, 0],X[y==1, 1]) plt.scatter(X[y==2, 0],X[y==2, 1]) plt.scatter(X[y==3, 0],X[y==3, 1]) plt.scatter(X[y==4, 0],X[y==4, 1])  \u0026lt;matplotlib.collections.PathCollection at 0x1a33248400\u0026gt;  print(y) y_cat = to_categorical(y,5) print(y_cat)  [3 3 4 0 2 1 1 4 4 4 3 2 1 0 2 4 4 4 3 0 3 0 3 4 1 3 1 0 4 2 3 1 0 4 1 0 4 0 4 0 1 3 3 0 3 1 3 4 0 4 0 2 3 0 2 4 4 0 3 4 1 3 0 4 2 1 1 2 3 2 3 2 1 1 3 2 4 2 2 3 0 2 4 3 2 1 3 3 3 3 1 1 4 2 4 4 1 2 4 1 4 2 2 3 4 1 1 3 3 2 1 3 1 0 1 0 4 4 4 3 1 3 3 3 1 4 4 0 2 0 1 1 0 4 0 1 1 4 4 0 4 3 4 4 2 4 0 0 0 4 2 0 4 1 0 2 2 1 3 0 1 2 2 4 4 1 3 2 3 3 3 2 2 2 4 0 1 1 0 1 4 4 3 2 3 2 3 1 0 3 4 2 1 0 0 0 4 3 0 4 2 2 3 1 0 4 4 1 2 1 3 1 4 4 0 1 4 2 4 0 4 0 1 2 3 0 1 0 3 3 4 3 4 0 0 1 1 3 0 4 1 4 2 1 2 4 4 3 3 1 1 1 4 1 1 1 4 3 1 0 0 2 2 2 3 2 3 4 2 3 3 1 1 0 4 0 3 0 0 0 3 2 2 1 4 2 0 0 2 2 1 3 3 1 3 0 0 1 4 2 2 2 1 0 0 1 0 0 1 2 4 2 4 2 1 3 0 0 1 4 1 2 4 4 4 3 4 1 2 3 1 3 1 0 0 2 4 1 3 1 1 2 1 4 3 1 2 3 2 0 4 1 4 0 2 2 0 1 2 2 4 4 0 4 1 3 3 2 2 0 4 3 3 1 2 2 4 3 0 3 3 3 3 0 1 4 4 4 0 3 3 2 4 0 4 0 0 4 2 2 2 3 0 2 0 0 1 2 3 0 0 2 4 2 3 0 3 0 1 0 4 1 0 0 1 3 1 4 0 2 1 1 3 2 3 2 2 2 3 1 0 0 0 1 3 4 2 1 0 2 1 4 3 3 4 0 0 2 2 0 0 2 2 3 4 3 2 2 2 2 0 1 3 0 2 1 2 0 2 3 4 3 1 4 3 3 0 2 4 0 4 1 3 2 1 3 0 1 2 1] [[0. 0. 0. 1. 0.] [0. 0. 0. 1. 0.] [0. 0. 0. 0. 1.] ... [0. 1. 0. 0. 0.] [0. 0. 1. 0. 0.] [0. 1. 0. 0. 0.]]  model = Sequential() model.add(Dense(units=5, input_shape=(2,), activation='softmax')) model.compile(Adam(0.1), loss='categorical_crossentropy', metrics=['accuracy'])  model.fit(x=X, y=y_cat, verbose=1, batch_size=50, epochs=100)  Epoch 1/100 500/500 [==============================] - 1s 1ms/step - loss: 1.2781 - acc: 0.4940 Epoch 2/100 500/500 [==============================] - 0s 49us/step - loss: 0.6296 - acc: 0.8520 Epoch 3/100 500/500 [==============================] - 0s 47us/step - loss: 0.4182 - acc: 0.9120 Epoch 4/100 500/500 [==============================] - 0s 65us/step - loss: 0.3323 - acc: 0.9440 Epoch 5/100 500/500 [==============================] - 0s 57us/step - loss: 0.2890 - acc: 0.9420 Epoch 6/100 500/500 [==============================] - 0s 50us/step - loss: 0.2626 - acc: 0.9460 Epoch 7/100 500/500 [==============================] - 0s 38us/step - loss: 0.2451 - acc: 0.9440 Epoch 8/100 500/500 [==============================] - 0s 51us/step - loss: 0.2323 - acc: 0.9480 Epoch 9/100 500/500 [==============================] - 0s 40us/step - loss: 0.2217 - acc: 0.9500 Epoch 10/100 500/500 [==============================] - 0s 150us/step - loss: 0.2145 - acc: 0.9500 Epoch 11/100 500/500 [==============================] - 0s 76us/step - loss: 0.2062 - acc: 0.9460 Epoch 12/100 500/500 [==============================] - 0s 72us/step - loss: 0.2001 - acc: 0.9440 Epoch 13/100 500/500 [==============================] - 0s 65us/step - loss: 0.1956 - acc: 0.9500 Epoch 14/100 500/500 [==============================] - 0s 47us/step - loss: 0.1914 - acc: 0.9500 Epoch 15/100 500/500 [==============================] - 0s 85us/step - loss: 0.1877 - acc: 0.9480 Epoch 16/100 500/500 [==============================] - 0s 64us/step - loss: 0.1857 - acc: 0.9460 Epoch 17/100 500/500 [==============================] - 0s 67us/step - loss: 0.1816 - acc: 0.9480 Epoch 18/100 500/500 [==============================] - 0s 60us/step - loss: 0.1811 - acc: 0.9420 Epoch 19/100 500/500 [==============================] - 0s 69us/step - loss: 0.1766 - acc: 0.9500 Epoch 20/100 500/500 [==============================] - 0s 79us/step - loss: 0.1767 - acc: 0.9460 Epoch 21/100 500/500 [==============================] - 0s 53us/step - loss: 0.1721 - acc: 0.9500 Epoch 22/100 500/500 [==============================] - 0s 57us/step - loss: 0.1725 - acc: 0.9460 Epoch 23/100 500/500 [==============================] - 0s 54us/step - loss: 0.1688 - acc: 0.9500 Epoch 24/100 500/500 [==============================] - 0s 69us/step - loss: 0.1693 - acc: 0.9440 Epoch 25/100 500/500 [==============================] - 0s 59us/step - loss: 0.1665 - acc: 0.9480 Epoch 26/100 500/500 [==============================] - 0s 91us/step - loss: 0.1666 - acc: 0.9460 Epoch 27/100 500/500 [==============================] - 0s 63us/step - loss: 0.1650 - acc: 0.9500 Epoch 28/100 500/500 [==============================] - 0s 68us/step - loss: 0.1637 - acc: 0.9520 Epoch 29/100 500/500 [==============================] - 0s 61us/step - loss: 0.1630 - acc: 0.9520 Epoch 30/100 500/500 [==============================] - 0s 76us/step - loss: 0.1634 - acc: 0.9440 Epoch 31/100 500/500 [==============================] - 0s 48us/step - loss: 0.1607 - acc: 0.9500 Epoch 32/100 500/500 [==============================] - 0s 85us/step - loss: 0.1613 - acc: 0.9480 Epoch 33/100 500/500 [==============================] - 0s 65us/step - loss: 0.1604 - acc: 0.9500 Epoch 34/100 500/500 [==============================] - 0s 58us/step - loss: 0.1604 - acc: 0.9480 Epoch 35/100 500/500 [==============================] - 0s 75us/step - loss: 0.1590 - acc: 0.9500 Epoch 36/100 500/500 [==============================] - 0s 52us/step - loss: 0.1590 - acc: 0.9500 Epoch 37/100 500/500 [==============================] - 0s 59us/step - loss: 0.1582 - acc: 0.9500 Epoch 38/100 500/500 [==============================] - 0s 60us/step - loss: 0.1575 - acc: 0.9500 Epoch 39/100 500/500 [==============================] - 0s 62us/step - loss: 0.1592 - acc: 0.9500 Epoch 40/100 500/500 [==============================] - 0s 58us/step - loss: 0.1581 - acc: 0.9460 Epoch 41/100 500/500 [==============================] - 0s 46us/step - loss: 0.1571 - acc: 0.9520 Epoch 42/100 500/500 [==============================] - 0s 55us/step - loss: 0.1562 - acc: 0.9520 Epoch 43/100 500/500 [==============================] - 0s 51us/step - loss: 0.1557 - acc: 0.9480 Epoch 44/100 500/500 [==============================] - 0s 57us/step - loss: 0.1567 - acc: 0.9500 Epoch 45/100 500/500 [==============================] - 0s 55us/step - loss: 0.1556 - acc: 0.9520 Epoch 46/100 500/500 [==============================] - 0s 45us/step - loss: 0.1571 - acc: 0.9480 Epoch 47/100 500/500 [==============================] - 0s 45us/step - loss: 0.1561 - acc: 0.9520 Epoch 48/100 500/500 [==============================] - 0s 42us/step - loss: 0.1555 - acc: 0.9440 Epoch 49/100 500/500 [==============================] - 0s 47us/step - loss: 0.1548 - acc: 0.9440 Epoch 50/100 500/500 [==============================] - 0s 49us/step - loss: 0.1543 - acc: 0.9440 Epoch 51/100 500/500 [==============================] - 0s 40us/step - loss: 0.1537 - acc: 0.9520 Epoch 52/100 500/500 [==============================] - 0s 45us/step - loss: 0.1539 - acc: 0.9500 Epoch 53/100 500/500 [==============================] - 0s 43us/step - loss: 0.1531 - acc: 0.9520 Epoch 54/100 500/500 [==============================] - 0s 47us/step - loss: 0.1550 - acc: 0.9460 Epoch 55/100 500/500 [==============================] - 0s 48us/step - loss: 0.1524 - acc: 0.9520 Epoch 56/100 500/500 [==============================] - 0s 36us/step - loss: 0.1541 - acc: 0.9520 Epoch 57/100 500/500 [==============================] - 0s 49us/step - loss: 0.1538 - acc: 0.9480 Epoch 58/100 500/500 [==============================] - 0s 43us/step - loss: 0.1530 - acc: 0.9480 Epoch 59/100 500/500 [==============================] - 0s 51us/step - loss: 0.1533 - acc: 0.9520 Epoch 60/100 500/500 [==============================] - 0s 38us/step - loss: 0.1528 - acc: 0.9520 Epoch 61/100 500/500 [==============================] - 0s 46us/step - loss: 0.1522 - acc: 0.9500 Epoch 62/100 500/500 [==============================] - 0s 45us/step - loss: 0.1526 - acc: 0.9500 Epoch 63/100 500/500 [==============================] - 0s 51us/step - loss: 0.1527 - acc: 0.9520 Epoch 64/100 500/500 [==============================] - 0s 52us/step - loss: 0.1522 - acc: 0.9500 Epoch 65/100 500/500 [==============================] - 0s 51us/step - loss: 0.1541 - acc: 0.9500 Epoch 66/100 500/500 [==============================] - 0s 41us/step - loss: 0.1521 - acc: 0.9500 Epoch 67/100 500/500 [==============================] - 0s 48us/step - loss: 0.1527 - acc: 0.9520 Epoch 68/100 500/500 [==============================] - 0s 53us/step - loss: 0.1533 - acc: 0.9500 Epoch 69/100 500/500 [==============================] - 0s 48us/step - loss: 0.1540 - acc: 0.9520 Epoch 70/100 500/500 [==============================] - 0s 46us/step - loss: 0.1542 - acc: 0.9460 Epoch 71/100 500/500 [==============================] - 0s 42us/step - loss: 0.1524 - acc: 0.9520 Epoch 72/100 500/500 [==============================] - 0s 42us/step - loss: 0.1535 - acc: 0.9500 Epoch 73/100 500/500 [==============================] - 0s 49us/step - loss: 0.1527 - acc: 0.9500 Epoch 74/100 500/500 [==============================] - 0s 47us/step - loss: 0.1518 - acc: 0.9480 Epoch 75/100 500/500 [==============================] - 0s 45us/step - loss: 0.1533 - acc: 0.9480 Epoch 76/100 500/500 [==============================] - 0s 39us/step - loss: 0.1516 - acc: 0.9500 Epoch 77/100 500/500 [==============================] - 0s 44us/step - loss: 0.1531 - acc: 0.9460 Epoch 78/100 500/500 [==============================] - 0s 45us/step - loss: 0.1555 - acc: 0.9480 Epoch 79/100 500/500 [==============================] - 0s 54us/step - loss: 0.1534 - acc: 0.9480 Epoch 80/100 500/500 [==============================] - 0s 48us/step - loss: 0.1527 - acc: 0.9460 Epoch 81/100 500/500 [==============================] - 0s 59us/step - loss: 0.1532 - acc: 0.9480 Epoch 82/100 500/500 [==============================] - 0s 66us/step - loss: 0.1520 - acc: 0.9480 Epoch 83/100 500/500 [==============================] - 0s 53us/step - loss: 0.1517 - acc: 0.9520 Epoch 84/100 500/500 [==============================] - 0s 55us/step - loss: 0.1524 - acc: 0.9500 Epoch 85/100 500/500 [==============================] - 0s 65us/step - loss: 0.1504 - acc: 0.9520 Epoch 86/100 500/500 [==============================] - 0s 63us/step - loss: 0.1518 - acc: 0.9500 Epoch 87/100 500/500 [==============================] - 0s 53us/step - loss: 0.1524 - acc: 0.9480 Epoch 88/100 500/500 [==============================] - 0s 51us/step - loss: 0.1512 - acc: 0.9480 Epoch 89/100 500/500 [==============================] - 0s 57us/step - loss: 0.1524 - acc: 0.9520 Epoch 90/100 500/500 [==============================] - 0s 62us/step - loss: 0.1524 - acc: 0.9500 Epoch 91/100 500/500 [==============================] - 0s 58us/step - loss: 0.1522 - acc: 0.9520 Epoch 92/100 500/500 [==============================] - 0s 62us/step - loss: 0.1516 - acc: 0.9480 Epoch 93/100 500/500 [==============================] - 0s 53us/step - loss: 0.1525 - acc: 0.9480 Epoch 94/100 500/500 [==============================] - 0s 61us/step - loss: 0.1510 - acc: 0.9520 Epoch 95/100 500/500 [==============================] - 0s 57us/step - loss: 0.1525 - acc: 0.9520 Epoch 96/100 500/500 [==============================] - 0s 38us/step - loss: 0.1504 - acc: 0.9480 Epoch 97/100 500/500 [==============================] - 0s 55us/step - loss: 0.1511 - acc: 0.9500 Epoch 98/100 500/500 [==============================] - 0s 56us/step - loss: 0.1516 - acc: 0.9500 Epoch 99/100 500/500 [==============================] - 0s 46us/step - loss: 0.1503 - acc: 0.9500 Epoch 100/100 500/500 [==============================] - 0s 61us/step - loss: 0.1518 - acc: 0.9480 \u0026lt;keras.callbacks.History at 0x1a332f0940\u0026gt;  def plot_decision_boundary(X, y_cat, model): x_span = np.linspace(min(X[:, 0]) - 1, max(X[:, 0]) + 1) y_span = np.linspace(min(X[:, 1]) - 1, max(X[:, 1]) + 1) xx, yy = np.meshgrid(x_span, y_span) grid = np.c_[xx.ravel(), yy.ravel()] pred_func = model.predict_classes(grid) z = pred_func.reshape(xx.shape) plt.contourf(xx, yy, z)  plot_decision_boundary(X, y_cat, model) plt.scatter(X[y==0, 0],X[y==0, 1]) plt.scatter(X[y==1, 0],X[y==1, 1]) plt.scatter(X[y==2, 0],X[y==2, 1]) plt.scatter(X[y==3, 0],X[y==3, 1]) plt.scatter(X[y==4, 0],X[y==4, 1]) x = 0.5 y = 0.5 point = np.array([[x, y]]) predict = model.predict_classes(point) plt.plot([x], [y], marker='o', markersize=10, color=\u0026quot;yellow\u0026quot;) # plt.show() print(\u0026quot;Prediction is: \u0026quot;, predict)  Prediction is: [3]  ","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://jianzhang6.github.io/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let's make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://jianzhang6.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://jianzhang6.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://jianzhang6.github.io/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]